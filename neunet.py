import pandas as pd
import numpy as np
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
import sklearn.svm as sksvm
import sklearn.model_selection as skm
import sklearn.feature_selection as skf
import sklearn.metrics as skmet
import sklearn.ensemble as ske
import sklearn.linear_model as skl




class Network(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_size[0])])
        layer_sizes = zip(hidden_size[: -1], hidden_size[1 :])
        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])
        self.output = nn.Linear(hidden_size[-1], output_size)

    def forward(self, x):
        for layer in self.hidden_layers:
            x = layer(x.float())
            x = F.relu(x)
        x = self.output(x)
        return torch.sigmoid(x)

def data(X, Y, batch_size):

    count = 0

    while count < X.shape[0]:
        yield X[count : count + batch_size, :], Y[count: count + batch_size]
        count += batch_size

def validation(model, dataset, criterion):
    test_loss = 0
    accuracy = 0
    count = 0
    for X, Y in dataset:
        X = torch.tensor(X)
        Y = torch.tensor(Y)
        count += X.shape[0]
        output = model.forward(X)
        test_loss += criterion(output, Y.float()).item()
        equality = (Y == (output.squeeze() >= 0.5).float())
        accuracy += equality.type(torch.FloatTensor).sum()
    accuracy = accuracy / count
    return test_loss, accuracy

def train(model, dataset, criterion, optimizer, batch_size, epoch, print_every):
    step = 0
    running_loss = 0

    for e in range(epoch):
#        train_dataset = data(X_train ,Y_train , batch_size)
#        test_dataset = data(X_test, Y_test, batch_size)
        for X, Y in dataset:
            optimizer.zero_grad()
            X = torch.tensor(X, requires_grad = True, dtype = float)
            Y = torch.tensor(Y)
            output = model.forward(X)
            loss = criterion(output.squeeze(), Y.float().squeeze())
            loss.backward()
            optimizer.step()
    return model
#            running_loss += loss.item()

#                with torch.no_grad():
#                    print(running_loss)
#                    running_loss = 0
#            step += 1
#        with torch.no_grad():
#            loss, acc = validation(model, test_dataset, criterion)
#            print("Loss ", loss, "Accuracy", acc)
model = Network(1470, [400], 1)
batch_size = 60
epoch = 3
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.02)
df = pd.read_csv('Features.csv')
feat = df.drop(df.columns[[0, -1]], axis = 1, inplace = False)
Xd = feat.values.astype(float)
Y = df['Class'].values
Yd = (Y == 'M').astype(float)
m1 = sksvm.SVC()
m2 = ske.RandomForestClassifier()
m3 = skl.LogisticRegression()


svmScore = 0
rfScore = 0
logScore = 0
nnscore = 0
for i in range(10):
    X_train, X_test, Y_train, Y_test = skm.train_test_split(Xd, Yd, test_size = 0.3)
    trainset = data(X_train, Y_train, batch_size)
    model = Network(1470, [400], 1)
    batch_size = 60
    epoch = 3
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr = 0.02)
    model = train(model, trainset, criterion, optimizer, batch_size, epoch, 10)
    m1.fit(X_train, Y_train)
    m2.fit(X_train, Y_train)
    m3.fit(X_train, Y_train)
    svmScore += m1.score(X_test, Y_test)
    rfScore += m2.score(X_test, Y_test)
    logScore += m3.score(X_test, Y_test)
    output = model.forward(torch.from_numpy(X_test))
    Y = torch.from_numpy(Y_test)
    score = (Y == (output.squeeze() >= 0.5).float())
    print(score.sum())
    nnscore += score.sum().item() / Y_test.shape[0]

print("SVM ", svmScore/10, "\nRF ", rfScore/10, "\nlog ", logScore/10, "\nnn ", nnscore/10)
